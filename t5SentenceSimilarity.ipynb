{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "t5SentenceSimilarity.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPmup1GYl/HHfAiYHH4wz8v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhijithneilabraham/sentence_similarity/blob/main/t5SentenceSimilarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJahXUZF9Vtj",
        "outputId": "cde14ea5-00b8-4aca-e15f-5f5ab28a34b6"
      },
      "source": [
        "!pip install transformers[tf-cpu]\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers[tf-cpu] in /usr/local/lib/python3.6/dist-packages (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers[tf-cpu]) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers[tf-cpu]) (0.9.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers[tf-cpu]) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers[tf-cpu]) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers[tf-cpu]) (0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers[tf-cpu]) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers[tf-cpu]) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers[tf-cpu]) (20.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers[tf-cpu]) (1.19.4)\n",
            "Requirement already satisfied: tensorflow-cpu>=2.0; extra == \"tf-cpu\" in /usr/local/lib/python3.6/dist-packages (from transformers[tf-cpu]) (2.4.0)\n",
            "Requirement already satisfied: onnxconverter-common; extra == \"tf-cpu\" in /usr/local/lib/python3.6/dist-packages (from transformers[tf-cpu]) (1.7.0)\n",
            "Requirement already satisfied: keras2onnx; extra == \"tf-cpu\" in /usr/local/lib/python3.6/dist-packages (from transformers[tf-cpu]) (1.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers[tf-cpu]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers[tf-cpu]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers[tf-cpu]) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers[tf-cpu]) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers[tf-cpu]) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers[tf-cpu]) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers[tf-cpu]) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers[tf-cpu]) (2.4.7)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu>=2.0; extra == \"tf-cpu\"->transformers[tf-cpu]) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu>=2.0; extra == \"tf-cpu\"->transformers[tf-cpu]) (1.12)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu>=2.0; extra == \"tf-cpu\"->transformers[tf-cpu]) (1.32.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu>=2.0; extra == \"tf-cpu\"->transformers[tf-cpu]) (2.4.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu>=2.0; extra == \"tf-cpu\"->transformers[tf-cpu]) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu>=2.0; extra == \"tf-cpu\"->transformers[tf-cpu]) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu>=2.0; extra == \"tf-cpu\"->transformers[tf-cpu]) (3.7.4.3)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu>=2.0; extra == \"tf-cpu\"->transformers[tf-cpu]) (2.10.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu>=2.0; extra == \"tf-cpu\"->transformers[tf-cpu]) (1.1.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu>=2.0; extra == \"tf-cpu\"->transformers[tf-cpu]) (0.36.2)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu>=2.0; extra == \"tf-cpu\"->transformers[tf-cpu]) (1.12.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu>=2.0; extra == \"tf-cpu\"->transformers[tf-cpu]) (0.3.3)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu>=2.0; extra == \"tf-cpu\"->transformers[tf-cpu]) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu>=2.0; extra == \"tf-cpu\"->transformers[tf-cpu]) (2.4.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu>=2.0; extra == \"tf-cpu\"->transformers[tf-cpu]) (0.10.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu>=2.0; extra == \"tf-cpu\"->transformers[tf-cpu]) (1.1.2)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.6/dist-packages (from onnxconverter-common; extra == \"tf-cpu\"->transformers[tf-cpu]) (1.8.0)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.6/dist-packages (from keras2onnx; extra == \"tf-cpu\"->transformers[tf-cpu]) (0.3.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-cpu>=2.0; extra == \"tf-cpu\"->transformers[tf-cpu]) (0.4.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-cpu>=2.0; extra == \"tf-cpu\"->transformers[tf-cpu]) (1.17.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-cpu>=2.0; extra == \"tf-cpu\"->transformers[tf-cpu]) (51.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-cpu>=2.0; extra == \"tf-cpu\"->transformers[tf-cpu]) (3.3.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-cpu>=2.0; extra == \"tf-cpu\"->transformers[tf-cpu]) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow-cpu>=2.0; extra == \"tf-cpu\"->transformers[tf-cpu]) (1.0.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-cpu>=2.0; extra == \"tf-cpu\"->transformers[tf-cpu]) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-cpu>=2.0; extra == \"tf-cpu\"->transformers[tf-cpu]) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-cpu>=2.0; extra == \"tf-cpu\"->transformers[tf-cpu]) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-cpu>=2.0; extra == \"tf-cpu\"->transformers[tf-cpu]) (4.2.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow-cpu>=2.0; extra == \"tf-cpu\"->transformers[tf-cpu]) (3.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-cpu>=2.0; extra == \"tf-cpu\"->transformers[tf-cpu]) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-cpu>=2.0; extra == \"tf-cpu\"->transformers[tf-cpu]) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow-cpu>=2.0; extra == \"tf-cpu\"->transformers[tf-cpu]) (3.4.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.94)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJ_ohhiz9ASo"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import transformers"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEY_yvHP9fQ0"
      },
      "source": [
        "max_length = 128  # Maximum length of input sentence to the model.\n",
        "batch_size = 512\n",
        "epochs = 2\n",
        "\n",
        "# Labels in our dataset.\n",
        "labels = [\"contradiction\", \"entailment\", \"neutral\"]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27xjmO5v9hBW",
        "outputId": "80940f0b-f0e5-42c5-c119-089296d90d29"
      },
      "source": [
        "!curl -LO https://raw.githubusercontent.com/MohamadMerchant/SNLI/master/data.tar.gz\n",
        "!tar -xvzf data.tar.gz"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 11.1M  100 11.1M    0     0  17.5M      0 --:--:-- --:--:-- --:--:-- 17.5M\n",
            "SNLI_Corpus/\n",
            "SNLI_Corpus/snli_1.0_dev.csv\n",
            "SNLI_Corpus/snli_1.0_train.csv\n",
            "SNLI_Corpus/snli_1.0_test.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4bh284M9l2F"
      },
      "source": [
        "train_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_train.csv\", nrows=100000)\n",
        "valid_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_dev.csv\")\n",
        "test_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_test.csv\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17UR2_t-9qiH"
      },
      "source": [
        "train_df.dropna(axis=0, inplace=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxqLN1Kv9vyG"
      },
      "source": [
        "train_df = (\n",
        "    train_df[train_df.similarity != \"-\"]\n",
        "    .sample(frac=1.0, random_state=42)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "valid_df = (\n",
        "    valid_df[valid_df.similarity != \"-\"]\n",
        "    .sample(frac=1.0, random_state=42)\n",
        "    .reset_index(drop=True)\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTVm9TXI91Gs"
      },
      "source": [
        "train_df[\"label\"] = train_df[\"similarity\"].apply(\n",
        "    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n",
        ")\n",
        "y_train = tf.keras.utils.to_categorical(train_df.label, num_classes=3)\n",
        "\n",
        "valid_df[\"label\"] = valid_df[\"similarity\"].apply(\n",
        "    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n",
        ")\n",
        "y_val = tf.keras.utils.to_categorical(valid_df.label, num_classes=3)\n",
        "\n",
        "test_df[\"label\"] = test_df[\"similarity\"].apply(\n",
        "    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n",
        ")\n",
        "y_test = tf.keras.utils.to_categorical(test_df.label, num_classes=3)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ODbBwkk96K5"
      },
      "source": [
        "class t5SemanticDataGenerator(tf.keras.utils.Sequence):\n",
        "    \"\"\"Generates batches of data.\n",
        "\n",
        "    Args:\n",
        "        sentence_pairs: Array of premise and hypothesis input sentences.\n",
        "        labels: Array of labels.\n",
        "        batch_size: Integer batch size.\n",
        "        shuffle: boolean, whether to shuffle the data.\n",
        "        include_targets: boolean, whether to incude the labels.\n",
        "\n",
        "    Returns:\n",
        "        Tuples `([input_ids, attention_mask, `token_type_ids], labels)`\n",
        "        (or just `[input_ids, attention_mask, `token_type_ids]`\n",
        "         if `include_targets=False`)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        sentence_pairs,\n",
        "        labels,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        include_targets=True,\n",
        "    ):\n",
        "        self.sentence_pairs = sentence_pairs\n",
        "        self.labels = labels\n",
        "        self.shuffle = shuffle\n",
        "        self.batch_size = batch_size\n",
        "        self.include_targets = include_targets\n",
        "        # Load our t5 Tokenizer to encode the text.\n",
        "        self.tokenizer = transformers.T5Tokenizer.from_pretrained(\n",
        "            \"t5-small\", do_lower_case=True\n",
        "        )\n",
        "        self.indexes = np.arange(len(self.sentence_pairs))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Denotes the number of batches per epoch.\n",
        "        return len(self.sentence_pairs) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Retrieves the batch of index.\n",
        "        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "        sentence_pairs = self.sentence_pairs[indexes]\n",
        "        encoded = self.tokenizer.batch_encode_plus(\n",
        "            sentence_pairs.tolist(),\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=True,\n",
        "            pad_to_max_length=True,\n",
        "            return_tensors=\"tf\",\n",
        "        )\n",
        "\n",
        "        # Convert batch of encoded features to numpy array.\n",
        "        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
        "        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
        "\n",
        "        # Set to true if data generator is used for training/validation.\n",
        "        if self.include_targets:\n",
        "            labels = np.array(self.labels[indexes], dtype=\"int32\")\n",
        "            return [input_ids, attention_masks, token_type_ids], labels\n",
        "        else:\n",
        "            return [input_ids, attention_masks, token_type_ids]\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # Shuffle indexes after each epoch if shuffle is set to True.\n",
        "        if self.shuffle:\n",
        "            np.random.RandomState(42).shuffle(self.indexes)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbwHBZy3-BEx",
        "outputId": "1bbcdae8-305e-42bb-efc5-347c521d158e"
      },
      "source": [
        "strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "with strategy.scope():\n",
        "    # Encoded token ids from t5 tokenizer.\n",
        "    input_ids = tf.keras.layers.Input(\n",
        "        shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n",
        "    )\n",
        "    # Attention masks indicates to the model which tokens should be attended to.\n",
        "    attention_masks = tf.keras.layers.Input(\n",
        "        shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n",
        "    )\n",
        "    # Token type ids are binary masks identifying different sequences in the model.\n",
        "    token_type_ids = tf.keras.layers.Input(\n",
        "        shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n",
        "    )\n",
        "    # Loading pretrained t5 model.\n",
        "    t5_model = transformers.TFT5Model.from_pretrained(\"t5-small\")\n",
        "    # Freeze the t5 model to reuse the pretrained features without modifying them.\n",
        "    t5_model.trainable = False\n",
        "\n",
        "    sequence_output = t5_model(\n",
        "        input_ids, attention_mask=attention_masks,decoder_input_ids=input_ids)[0]\n",
        "    # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n",
        "    bi_lstm = tf.keras.layers.Bidirectional(\n",
        "        tf.keras.layers.LSTM(64, return_sequences=True)\n",
        "    )(sequence_output)\n",
        "    # Applying hybrid pooling approach to bi_lstm sequence output.\n",
        "    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n",
        "    max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n",
        "    concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n",
        "    dropout = tf.keras.layers.Dropout(0.3)(concat)\n",
        "    output = tf.keras.layers.Dense(3, activation=\"softmax\")(dropout)\n",
        "    model = tf.keras.models.Model(\n",
        "        inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"acc\"],\n",
        "    )\n",
        "\n",
        "\n",
        "print(f\"Strategy: {strategy}\")\n",
        "model.summary()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFT5Model.\n",
            "\n",
            "All the layers of TFT5Model were initialized from the model checkpoint at t5-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5Model for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f260d75f730>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: <cyfunction Socket.send at 0x7f2624f9ae58> is not a module, class, method, function, traceback, frame, or code object\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f260d75f730>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: <cyfunction Socket.send at 0x7f2624f9ae58> is not a module, class, method, function, traceback, frame, or code object\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f26229418c8> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function wrap at 0x7f26229418c8> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Strategy: <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7f25870099e8>\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "attention_masks (InputLayer)    [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_t5model (TFT5Model)          TFSeq2SeqModelOutput 60506624    input_ids[0][0]                  \n",
            "                                                                 attention_masks[0][0]            \n",
            "                                                                 input_ids[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   (None, 128, 128)     295424      tf_t5model[0][1]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d (Globa (None, 128)          0           bidirectional[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d (GlobalMax (None, 128)          0           bidirectional[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 256)          0           global_average_pooling1d[0][0]   \n",
            "                                                                 global_max_pooling1d[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dropout_62 (Dropout)            (None, 256)          0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "token_type_ids (InputLayer)     [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 3)            771         dropout_62[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 60,802,819\n",
            "Trainable params: 296,195\n",
            "Non-trainable params: 60,506,624\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIKw4dxYR8Js"
      },
      "source": [
        "train_data = t5SemanticDataGenerator(\n",
        "    train_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"),\n",
        "    y_train,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "valid_data = t5SemanticDataGenerator(\n",
        "    valid_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"),\n",
        "    y_val,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3RecBzsSCQO",
        "outputId": "0647f984-76f5-458d-ac9d-fa564fb2b8fe"
      },
      "source": [
        "history = model.fit(\n",
        "    train_data,\n",
        "    validation_data=valid_data,\n",
        "    epochs=epochs,\n",
        "    use_multiprocessing=True,\n",
        "    workers=-1,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2179: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  2/195 [..............................] - ETA: 7:05:53 - loss: 1.1718 - acc: 0.3086"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}